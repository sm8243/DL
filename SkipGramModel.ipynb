{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax0HD0_I0lQL",
        "outputId": "7c52b1ee-96ef-48cd-d2c6-fbb5eb902923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100] Loss: 25.3676\n",
            "Epoch [20/100] Loss: 18.4093\n",
            "Epoch [30/100] Loss: 15.5657\n",
            "Epoch [40/100] Loss: 14.2045\n",
            "Epoch [50/100] Loss: 13.6089\n",
            "Epoch [60/100] Loss: 13.2303\n",
            "Epoch [70/100] Loss: 12.8657\n",
            "Epoch [80/100] Loss: 12.6795\n",
            "Epoch [90/100] Loss: 12.5705\n",
            "Epoch [100/100] Loss: 12.4006\n",
            "Words similar to 'fox': ['jumps', 'quick', 'over', 'brown', 'the']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Sample text data (tokenized)\n",
        "corpus = \"the quick brown fox jumps over the lazy dog\".split()\n",
        "\n",
        "# Create a vocabulary and word-to-index mapping\n",
        "vocab = set(corpus)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 50\n",
        "context_window = 1  # Range of words to consider before and after the current word\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create training data in the form of (target, context) pairs\n",
        "data = []\n",
        "for i, target_word in enumerate(corpus):\n",
        "    for j in range(max(0, i - context_window), min(i + context_window + 1, len(corpus))):\n",
        "        if i != j:\n",
        "            data.append((word_to_idx[target_word], word_to_idx[corpus[j]]))\n",
        "\n",
        "# Define the Skip-Gram model\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, target):\n",
        "        embedded = self.embeddings(target)\n",
        "        predicted = self.linear(embedded)\n",
        "        return predicted\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "model = SkipGram(vocab_size, embedding_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    np.random.shuffle(data)\n",
        "    for target, context in data:\n",
        "        target_tensor = torch.LongTensor([target])\n",
        "        context_tensor = torch.LongTensor([context])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(target_tensor)\n",
        "        loss = loss_fn(output, context_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Get word embeddings\n",
        "word_embeddings = model.embeddings.weight.detach().numpy()\n",
        "\n",
        "# Example usage: Find similar words\n",
        "def find_similar_words(word, top_n=5):\n",
        "    if word in word_to_idx:\n",
        "        word_vector = word_embeddings[word_to_idx[word]]\n",
        "        cosine_similarities = np.dot(word_embeddings, word_vector) / (\n",
        "            np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_vector)\n",
        "        )\n",
        "        most_similar_idx = np.argsort(cosine_similarities)[::-1][1:top_n + 1]\n",
        "        most_similar_words = [idx_to_word[idx] for idx in most_similar_idx]\n",
        "        return most_similar_words\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Example usage:\n",
        "target_word = \"fox\"\n",
        "similar_words = find_similar_words(target_word)\n",
        "print(f\"Words similar to '{target_word}': {similar_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "baWsJgAg0z0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}